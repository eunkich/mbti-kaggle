{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8eb608e3-4a08-4962-be7e-f53ea5aa1421",
    "_uuid": "d42dbe67549b6bb80d80d2ff8b613733f1c84d30"
   },
   "outputs": [],
   "source": [
    "This kernel explores the Myers-Briggs Personality Type Dataset. The data was extracted from comments let by users with different MBTI personality.\n",
    "\n",
    "I used a simple model which trains relatively quickly. The cross validation output an f1-score of ~0.65. It is possible to search for hyper paramaters with bayesian optimisation to improve accuracy. Also, a pre-trained words embedding like glove in an neural model may bring more insight. A lot of the links are dead links but it would be interesting to scrap the different urls to extract more information. \n",
    "\n",
    "# Text Analysis with (MBTI) Myers-Briggs Personality Type Dataset\n",
    "\n",
    "## Description \n",
    "\n",
    "\n",
    "\n",
    "Ii is implemented in Python 3.6.\n",
    "\n",
    "The main steps are :\n",
    "\n",
    "1. Treat data\n",
    "* Load data using pandas\n",
    "* Split each row by type | comments \n",
    "* Cleaning each comments by :\n",
    "    * removing all urls replaced by dummy word 'link' -> erase (replace by '')\n",
    "    * removing everything except letters\n",
    "    * removing larger spaces and lowering\n",
    "    * removing stopwords and lemmatizing\n",
    "    * Labelizing each mbti personality\n",
    "\n",
    "2. Learning phase:\n",
    "    - 16 classes (mutliclass):\n",
    "        * Run tf-idf and count vectorization of words vectorization \n",
    "        * Compute pca, fast\\_ica, tsne\n",
    "        * Multinomial Naive Bayes (F1-score ~ 0.603) [5 fold stratified cross validation]\n",
    "        * XGboost (F1-score ~ 0.656) [5 fold stratified cross validation]\n",
    "        \n",
    "    - 4 binary classes (mutlioutput binary classification):\n",
    "        * 4 Adaboost model with a multioutput classifier\n",
    "        \n",
    "As the letters are only paired by two, the problem is to predict 4 binary variables. The cross validation gives the following table. \n",
    "          \n",
    "|Attribute| Extraversion (E) - Introversion (I)  | Sensation (S) - INtuition (N)  | Thinking (T) - Feeling (F)  | Judgement (J) - Perception (P)  |   \n",
    "|:-:|---|---|---|---|\n",
    "|F1-Score|0.59 | 0.44 | 0.8 |  0.83| \n",
    "\n",
    "This allows us to see what are the axes that the algorithm may fail at identifying (S Sensation vs N Intuition).\n",
    "\n",
    "Finally there are two neural networks, (1D convnet and LSTM) but I did not have the time to tune them.\n",
    "\n",
    "TO DO:\n",
    "- Scrape urls and extract topic of pages\n",
    "- Balance the dataset with resampling techniques\n",
    "- Hyper Parameter Optimization\n",
    "- Convolutional Network with glove embedding\n",
    "- Search multiple classifiers (not correlated) and stack them and build pipeline with Lasso."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "0e10c6de-685a-4e93-9925-7d73cb27497c",
    "_uuid": "a0f7e95a8994cea779577c48c3d22cde3fdf2cc5"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv('../input/mbti_1.csv') # dtype = {'type': str,'post': ,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "376de180-026c-46c1-8dfa-e2550edd0955",
    "_uuid": "840ae5aa20a5c29b983be8c45928862c299b9792"
   },
   "source": [
    "## Overview of the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "315f710c-371f-4c9d-87f0-13fbc0e29047",
    "_uuid": "03a18534be1141ebff5875c48f7075995f23ca60"
   },
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "bda2db8c-c9c6-446b-a40e-f86241d60619",
    "_uuid": "2d145f588ec28c76d0c757d53e3b657e21c4e7a9"
   },
   "source": [
    "### List of posts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "05abe1d2-7acd-4f20-9539-be413c76822b",
    "_uuid": "14c6ff28cd51d9969324fecb6a53b05ca5584069"
   },
   "source": [
    "[p.split('|||') for p in data.head(1).posts.values]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "24c37003-e27e-4278-aa92-0884c595f1b3",
    "_uuid": "79b9bb46f0aeba9d734a35bfdffedb7ad90ff51c"
   },
   "source": [
    "# Distribution of the target variable?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "1abc712c-49df-4316-ae70-c2e6a68c11e0",
    "_uuid": "2fc67d1db4ef61d5998b262ea0cf4eae9bbe8594"
   },
   "source": [
    "plt.figure(figsize=(40,20))\n",
    "plt.xticks(fontsize=24, rotation=0)\n",
    "plt.yticks(fontsize=24, rotation=0)\n",
    "sns.countplot(data=data, x='type')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "ad38422a-6742-41f5-ab7c-13f3dd78880b",
    "_uuid": "6eefe6923a02aeeefc0dcde74ee0a914f41bb73b"
   },
   "source": [
    "#### It is clearly unbalanced throughout the different classes. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "dbcabf3e-06a9-4bf4-88d8-a16690a12c11",
    "_uuid": "1832a12c5350356c50d8664aeb4cecd68adc6026"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "a2fbfbc6-4aea-4202-acd5-ad5a07d61fb4",
    "_uuid": "196c5662911d0f7c33a49773138c6506d9366fdf"
   },
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "99d84362-6521-45a4-9c14-3d236d154bae",
    "_uuid": "a66303b0a5f658ae3a4852224659a706879b59bc"
   },
   "source": [
    "##### Encode each type to an int\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "lab_encoder = LabelEncoder().fit(unique_type_list)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "5840c3c9-2e64-4e85-bdb5-cddd64c17ebc",
    "_uuid": "6dff206a4e913892839455b75ffc22de56320743"
   },
   "source": [
    "### Posts cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "84616491-994b-4659-b515-9b92fe707add",
    "_uuid": "29ccab43c1c8eb2ab4caa3085513f7e56d69c7a4"
   },
   "source": [
    "import time\n",
    "##### Compute list of subject with Type | list of comments \n",
    "\n",
    "# Time\n",
    "%time data.posts[1].replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ')\n",
    "%time re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', data.posts[1])\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Lemmatizer | Stemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Cache the stop words for speed \n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "# One post\n",
    "OnePost = data.posts[1]\n",
    "\n",
    "# List all urls\n",
    "urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', OnePost)\n",
    "\n",
    "# Remove urls\n",
    "temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', OnePost)\n",
    "\n",
    "# Keep only words\n",
    "temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "\n",
    "# Remove spaces > 1\n",
    "temp = re.sub(' +', ' ', temp).lower()\n",
    "\n",
    "# Remove stopwords and lematize\n",
    "%time stemmer.stem(\" \".join([w for w in temp.split(' ') if w not in cachedStopWords]))\n",
    "\n",
    "print(\"\\nBefore preprocessing:\\n\\n\", OnePost[0:500])\n",
    "print(\"\\nAfter preprocessing:\\n\\n\", temp[0:500])\n",
    "print(\"\\nList of urls:\")\n",
    "urls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "7902b557-a441-4524-8e10-8b7c463b8c5a",
    "_uuid": "b6d6b98ad50f4590bb790f31ab275ac3eab7c169"
   },
   "source": [
    "### Preprocessing comments\n",
    "\n",
    "* Replace urls with a dummy word: \"link\"\n",
    "* Keep only words and put everything lowercase\n",
    "* Lemmatize each word \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "733a2aed-6588-4948-9736-4efa07188943",
    "_uuid": "3bd6a4ff38bbe36b4f80b92d66c67cf9d8acce02"
   },
   "source": [
    "##### Compute list of subject with Type | list of comments \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Lemmatize\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "def pre_process_data(data, remove_stop_words=True):\n",
    "\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    len_data = len(data)\n",
    "    i=0\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        i+=1\n",
    "        if i % 500 == 0:\n",
    "            print(\"%s | %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', posts)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp).lower()\n",
    "        if remove_stop_words:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n",
    "        else:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "\n",
    "        type_labelized = lab_encoder.transform([row[1].type])[0]\n",
    "        list_personality.append(type_labelized)\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    #del data\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality\n",
    "\n",
    "list_posts, list_personality = pre_process_data(data, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "5935f4ca-6d8d-40ae-850c-fd4409968fd8",
    "_uuid": "50c7d4b18f04b6868bc235b5ef0ac6d5a2d22032"
   },
   "source": [
    "### Vectorize with count and tf-idf\n",
    "\n",
    "I kept the words appearing in 10 to 50% of the posts. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "ea8bb4b1-0f28-4642-924d-4379c778abbf",
    "_uuid": "fa17dad79c85809225b1ae29b18a3a1501a3f574"
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "cntizer = CountVectorizer(analyzer=\"word\", \n",
    "                             max_features=1500, \n",
    "                             tokenizer=None,    \n",
    "                             preprocessor=None, \n",
    "                             stop_words=None,  \n",
    "#                             ngram_range=(1,1),\n",
    "                             max_df=0.5,\n",
    "                             min_df=0.1) \n",
    "                                 \n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "print(\"CountVectorizer\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "print(\"Tf-idf\")\n",
    "X_tfidf =  tfizer.fit_transform(X_cnt).toarray()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_cnt[0][0][]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "4563d9df-3c32-45fb-ac64-103f22aae383",
    "_uuid": "763745c5bf4ccac6aa8c84cb937fb928ce8df647"
   },
   "source": [
    "list_posts[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "100f2063-efe5-448c-809a-2ead70190381",
    "_uuid": "f02c5786ce7714a179721417e0463117ea23d1e7"
   },
   "source": [
    "### Count the top 50 words of the count vectorizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "ec9b2fa7-da8e-4eea-ac73-e898f9be3943",
    "_uuid": "36e4d1703e964eb0d8f76e8547e4a6a7c21ad6af"
   },
   "source": [
    "reverse_dic = {}\n",
    "for key in cntizer.vocabulary_:\n",
    "    reverse_dic[cntizer.vocabulary_[key]] = key"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "72ddbfca-a4e4-4506-8474-4345767b9e39",
    "_uuid": "7d7c435788ba0b86ddbcbebbd51daf3431ca2f67",
    "scrolled": true
   },
   "source": [
    "top_50 = np.asarray(np.argsort(np.sum(X_cnt, axis=0))[0,-50:][0, ::-1]).flatten()\n",
    "[reverse_dic[v] for v in top_50]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "4f208b76-82df-4383-b6b7-4afb86e75e6c",
    "_uuid": "2aeddf0e1517e02d0b0c4bbe49e518baaa3772f6"
   },
   "source": [
    "The 4 top words are actually some mbti profiles... This is strange!\n",
    "\n",
    "Lets try dimensionality reduction:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "8a707754-fe83-49f4-b561-99942bab5e3f",
    "_uuid": "11b23e9d54343717bcf6fe9da77b9aa90f46f8aa"
   },
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Truncated SVD\n",
    "svd = TruncatedSVD(n_components=12, n_iter=7, random_state=42)\n",
    "svd_vec = svd.fit_transform(X_tfidf)\n",
    "\n",
    "print(\"TSNE\")\n",
    "X_tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=650).fit_transform(svd_vec)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "23f07804-9233-4a62-b3d2-ca63a62be082",
    "_uuid": "a5685baf6df806ab6e44531e1031b034b9d1981e"
   },
   "source": [
    "### Plot tsne with 3 components"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_tsne"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "500f677f-2f50-46d6-a781-a79cbdff2bff",
    "_uuid": "aee9a6f532c16a8c49d8e0a3b9a46c1576d7fa31"
   },
   "source": [
    "col = list_personality\n",
    "plt.figure(0)\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=col, cmap=plt.get_cmap('tab20') , s=12)\n",
    "plt.figure(1)\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,2], c=col, cmap=plt.get_cmap('tab20') , s=12)\n",
    "plt.figure(2)\n",
    "plt.scatter(X_tsne[:,1], X_tsne[:,2], c=col, cmap=plt.get_cmap('tab20') , s=12)\n",
    "legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "44215420-71e0-4626-96b5-67a99322ee28",
    "_uuid": "4e617fa8a27d8fa54e46fd86b600cd3e20fa7b30"
   },
   "source": [
    "### Plot first axes of decomposition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "a50fbe38-2d0a-47c6-a5b5-35eaa63a41c4",
    "_uuid": "8c8829e9ed4a36eba9cb87774b71d5cacabca9cd",
    "scrolled": true
   },
   "source": [
    "from sklearn.decomposition import KernelPCA, FastICA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PCA\n",
    "pca_vec = PCA(n_components=12).fit_transform(X_tfidf)\n",
    "\n",
    "# ICA\n",
    "ica_vec = FastICA(n_components=12).fit_transform(X_tfidf)\n",
    "\n",
    "# Plot\n",
    "plt.figure(1)\n",
    "plt.scatter(pca_vec[:,0], pca_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)\n",
    "plt.figure(2)\n",
    "plt.scatter(svd_vec[:,0], svd_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)\n",
    "plt.figure(3)\n",
    "plt.scatter(ica_vec[:,0], ica_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "26355d5e-3a87-40e1-9b23-819bf8d7c2ec",
    "_uuid": "5c216341ece64a8c745a8ca68dfd06fdd9dd9d28"
   },
   "source": [
    "#### Plot tsne for each pair of letter:\n",
    "\n",
    "* Extraversion (E) - Introversion (I)\n",
    "* Sensation (S) - INtuition (N)\n",
    "* Thinking (T) - Feeling (F)\n",
    "* Judgement (J) - Perception (P)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "18197bbd-5016-4833-a3cd-e6f45605d30d",
    "_uuid": "85b5d10a691364b224656410220df047b386874c",
    "scrolled": false
   },
   "source": [
    "# Split mbti personality into 4 letters and binarize\n",
    "titles = [\"Extraversion (E) - Introversion (I)\",\n",
    "          \"Sensation (S) - INtuition (N)\",\n",
    "          \"Thinking (T) - Feeling (F)\",\n",
    "          \"Judgement (J) - Perception (P)\"\n",
    "         ] \n",
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    '''\n",
    "    transform mbti to binary vector\n",
    "    '''\n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "def translate_back(personality):\n",
    "    '''\n",
    "    transform binary vector to mbti personality\n",
    "    '''\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)\n",
    "\n",
    "# Plot\n",
    "def plot_tsne(X, i):\n",
    "    a = plt.figure(i, figsize=(30,20))\n",
    "    plt.title(titles[i])\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.scatter(X[:,0], X[:,1], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.scatter(X[:,0], X[:,2], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.scatter(X[:,1], X[:,2], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "e0790fa1-adbd-4ba0-b541-c677ad25a50b",
    "_uuid": "0f4d91d2d1fa8cca42a42605843da58435e2c7e0"
   },
   "source": [
    "#### Extraversion (E) - Introversion (I)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "fe876c32-d095-49f7-b935-2ec5103765c2",
    "_uuid": "4dda84494d50afe4474538902739883a79e80ead"
   },
   "source": [
    "plot_tsne(X_tsne, 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "fae26bc6-a062-4950-a1bd-8ccf5ebb8fcb",
    "_uuid": "f66a0097ea59504595729c39b05908345048b885"
   },
   "source": [
    "#### Sensation (S) - INtuition (N)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "f2cdc32a-77e1-428a-8e22-e2967535edf9",
    "_uuid": "b1e233fbae9e7a236f237e8389356da10169acd0"
   },
   "source": [
    "plot_tsne(X_tsne, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "f3084bf5-cdbc-40d0-8bce-952cb1ee0a86",
    "_uuid": "d6459545fc7e2f21a83caee43feeea35ab52febb"
   },
   "source": [
    "#### Thinking (T) - Feeling (F)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "01dfb487-39d0-4a9f-ba40-da670c80beeb",
    "_uuid": "9366f085027c903afd09419a3e156b5e9c146970"
   },
   "source": [
    "plot_tsne(X_tsne, 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "60b5940a-7afd-4676-90f6-214562987c6a",
    "_uuid": "0b225a198324fd1da2810245d61a21cf60fa7a9d"
   },
   "source": [
    "#### Judgement (J) - Perception (P)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "06c08c87-fbb0-43a9-bfb8-a9a4b52a7c3f",
    "_uuid": "d02a90c52b7eaadec522f160d0d3935034ba61bb"
   },
   "source": [
    "plot_tsne(X_tsne, 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "6d842d66-e53e-486d-a10d-c66e3c7f1bce",
    "_uuid": "19399ccc46cefc8b7533471d75ae6b76333bbf38"
   },
   "source": [
    "#### Confusion plot function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "f83939a4-139d-4671-a400-f93b7f049478",
    "_uuid": "f96f3440b4d5e74f11c76e12b3596c820e261c1c"
   },
   "source": [
    "# Confusion plot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "f777dbca-1af5-41ca-885d-f866d1d0bfdc",
    "_uuid": "4f3de7448eb2d0f6710d2823c44f7842e3cc2273"
   },
   "source": [
    "### Try multiple sklearn classifiers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "61ae0dd4-59ea-4dde-92c4-e7e09358abed",
    "_uuid": "92222f6f57dab9f7bd9135c29d6de9c9e0ad6d50",
    "collapsed": true
   },
   "source": [
    "##### Sklearn classifiers\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Vectorizer\n",
    "\n",
    "cntizer = CountVectorizer(analyzer=\"word\", \n",
    "                             max_features=1000, \n",
    "                             tokenizer=None,    \n",
    "                             preprocessor=None, \n",
    "                             stop_words=None,   \n",
    "                             max_df=0.5,\n",
    "                             min_df=0.1) \n",
    "\n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "# Classifiers\n",
    "PassAgg = PassiveAggressiveClassifier(max_iter=50)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge',   \n",
    "              penalty='l1',   \n",
    "              alpha=1e-2,     \n",
    "              random_state=42,\n",
    "              max_iter=7,     \n",
    "              tol=None)\n",
    "\n",
    "# SVM\n",
    "lsvc = LinearSVC()\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mlNB = MultinomialNB()\n",
    "\n",
    "# Xgboost \n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softprob'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.6\n",
    "param['ntrees'] = 300\n",
    "param['subsample'] = 0.93\n",
    "param['max_depth'] = 2\n",
    "param['silent'] = 1\n",
    "param['n_jobs'] = 8\n",
    "param['num_class'] = len(unique_type_list)\n",
    "xgb_class = xgb.XGBClassifier(**param)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "829a1c71-11a5-4373-bdf0-52b30bd47e60",
    "_uuid": "113df75f59d260437089bb76b70484ede6f74fbd"
   },
   "source": [
    "### Stratified K-fold validation training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "f89d9440-a23c-4b0b-9ce4-5548595b5f1e",
    "_uuid": "680baf0d4cac90d2ce061e73e49e27f2154fae7d",
    "collapsed": true
   },
   "source": [
    "# Train with k fold stratified validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "name = lambda x : str(x).split('(')[0]\n",
    "\n",
    "def train_stratified(models, X, y, add_idf=False, nsplits=3, confusion=False):\n",
    "    '''\n",
    "    Take a sklearn model like, feature set X, target set y and number of splits to compute Stratified kfold validation.\n",
    "    Args:\n",
    "        X (array):       Numpy array of features.\n",
    "        y (str):         Target - Personality list.\n",
    "        add_idf (bool):  Wehther to use tf-idf on CountVectorizer.\n",
    "        nsplits(int):    Number of splits for cross validation.\n",
    "        confusion(bool): Wether to plot confusion matrix \n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionnary of classifiers and their cv f1-score.\n",
    "    '''\n",
    "    fig_i = 0\n",
    "    kf = StratifiedShuffleSplit(n_splits=nsplits)\n",
    "    \n",
    "    # Store fold score for each classifier in a dictionnary\n",
    "    dico_score = {}\n",
    "    dico_score['merged'] = 0\n",
    "    for model in models:\n",
    "        dico_score[name(model)] = 0\n",
    "    \n",
    "    # Stratified Split\n",
    "    for train, test in kf.split(X,y):\n",
    "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "        \n",
    "        X_train = cntizer.fit_transform(X_train)\n",
    "        X_test = cntizer.transform(X_test)\n",
    "        \n",
    "        # tf-idf\n",
    "        if add_idf == True:\n",
    "            X_train_tfidf = tfizer.fit_transform(X_train_cnt)\n",
    "            X_test_tfidf = tfizer.transform(X_test_cnt)\n",
    "        \n",
    "            X_train = np.column_stack((X_train_tfidf.todense(), X_train))\n",
    "            X_test = np.column_stack((X_test_tfidf.todense(), X_test))\n",
    "        \n",
    "        probs = np.ones((len(y_test), 16))\n",
    "        for model in models:\n",
    "            # if xgboost use dmatrix\n",
    "            if 'XGB' in name(model):\n",
    "                xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "                xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "                watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "                num_round = 30\n",
    "                bst = xgb.train(param, xg_train, num_round, watchlist, early_stopping_rounds=6)\n",
    "                preds = bst.predict(xg_test)\n",
    "                probs = np.multiply(probs, preds)\n",
    "                preds = np.array([np.argmax(prob) for prob in preds])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                preds = model.predict(X_test)\n",
    "                probs = np.multiply(probs, model.predict_proba(X_test))\n",
    "            # f1-score\n",
    "            score = f1_score(y_test, preds, average='weighted')\n",
    "            dico_score[name(model)] += score\n",
    "            print('%s : %s' % (str(model).split('(')[0], score))\n",
    "            \n",
    "            if confusion == True:\n",
    "                # Compute confusion matrix\n",
    "                cnf_matrix = confusion_matrix(y_test, preds)\n",
    "                np.set_printoptions(precision=2)\n",
    "                # Plot confusion matrix\n",
    "                plt.figure(fig_i)\n",
    "                fig_i += 1\n",
    "                plot_confusion_matrix(cnf_matrix, classes=lab_encoder.inverse_transform(range(16)), normalize=True,\n",
    "                                                          title=('Confusion matrix %s' % name(model)))\n",
    "        \n",
    "        # product of class probabilites of each classifier \n",
    "        merged_preds = [np.argmax(prob) for prob in probs]\n",
    "        score = f1_score(y_test, merged_preds, average='weighted')\n",
    "        print('Merged score: %s' % score)\n",
    "        dico_score['merged'] += score\n",
    "        \n",
    "    return {k: v / nsplits for k, v in dico_score.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "9e6a38f2-7d8e-4512-bdf4-85d230855ec4",
    "_uuid": "29e0c7b0a8c475246cdad26791ad6db0a254e096"
   },
   "source": [
    "#### Compare multinomial naive bayes, xgb and their product predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "393632cf-22cf-49b4-97ca-88cb3c6cd2ba",
    "_uuid": "b766aeeede25e41b209e1f6ac8c8499928673e38",
    "scrolled": false
   },
   "source": [
    "results = train_stratified([mlNB, xgb_class], list_posts, list_personality, add_idf=False, nsplits=5, confusion=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "31e80231-fcea-45ed-b4ba-0a3a2b71576d",
    "_uuid": "da41ee2ae73e7306d62bd8b259da20fc4114058c"
   },
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "ca37c745-8736-4f5c-96ee-e67815141c6a",
    "_uuid": "5808ce84c622a30d92f57b2a0a14a394564bd37f"
   },
   "source": [
    "### Try multioutput classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "6fcfa0cb-5639-4c4e-b29f-2e08795c1242",
    "_uuid": "4fc1c0c9b6da2b44d709d5d88593a54257697303",
    "scrolled": true
   },
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    '''\n",
    "    transform mbti to binary vector\n",
    "    '''\n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "def translate_back(personality):\n",
    "    '''\n",
    "    transform binary vector to mbti personality\n",
    "    '''\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "8c37591b-983b-4044-9244-91980aefd99f",
    "_uuid": "6880e52e889124067e3f1f2cab60a1202027150c",
    "collapsed": true
   },
   "source": [
    "# Feed classifier to MultiOutputCLassifier\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "multi_target_classifier = MultiOutputClassifier(clf, n_jobs=-1)\n",
    "multi_target_classifier.fit(X_tfidf, list_personality_bin)\n",
    "preds = multi_target_classifier.predict(X_tfidf)\n",
    "\n",
    "preds_t = [translate_back(p) for p in preds]\n",
    "vec1 = data.type ==  preds_t\n",
    "for i in range(4):\n",
    "    print(\"f1 score for %s:\\n%s\" % (titles[i],\n",
    "                                    f1_score(np.array(list_personality_bin)[:,i], preds[:,i])))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "6cbbdf3c-e9cc-4799-ac9a-b4797660775a",
    "_uuid": "c06928ae12a17e4f7c08b32fe6fa3b945714d11f",
    "scrolled": true
   },
   "source": [
    "# Stratified cross val for multi-output\n",
    "X = list_posts\n",
    "y = np.array(list_personality_bin)\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "kf = StratifiedShuffleSplit(n_splits=4)\n",
    "\n",
    "list_score = []\n",
    "list_score_per_class= []\n",
    "\n",
    "for train, test in kf.split(X, y):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        X[train], X[test], y[train], y[test]\n",
    "\n",
    "    X_train = cntizer.fit_transform(X_train)\n",
    "    X_test = cntizer.transform(X_test)\n",
    "    \n",
    "    X_train = tfizer.fit_transform(X_train).toarray()\n",
    "    X_test = tfizer.transform(X_test).toarray()\n",
    "\n",
    "    multi_target_classifier = MultiOutputClassifier(clf, n_jobs=-1)\n",
    "    multi_target_classifier.fit(X_train, y_train)\n",
    "    preds = multi_target_classifier.predict(X_test)\n",
    "    \n",
    "    rev_preds = np.array([translate_back(p) for p in preds]) \n",
    "    rev_test = np.array([translate_back(p) for p in y_test])\n",
    "    score = f1_score(rev_test,rev_preds, average='weighted')\n",
    "    list_score.append(score)\n",
    "    print('\\nTotal score: %s' % f1_score(rev_test,rev_preds, average='weighted'))\n",
    "\n",
    "    list_temp =[]\n",
    "    for i in range(4):\n",
    "        score_per_class = f1_score(y_test[:,i], preds[:,i])\n",
    "        list_temp.append(score_per_class)\n",
    "        print(score_per_class)\n",
    "    list_score_per_class.append(list_temp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "b5464e41-7ae9-4d2e-b847-ab1ce17955fd",
    "_uuid": "7e8a32f1524a3e0c4d6a181b399255859923efc4"
   },
   "source": [
    "list_score_per_class = np.array(list_score_per_class)\n",
    "print('Mean score per classes: %s' % list_score_per_class.mean(axis=0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "ce8bacb9-9418-4bec-968e-252eb083b30b",
    "_uuid": "e1521d625c318866b593c3b1dcd39042c00bbff0"
   },
   "source": [
    "## Neural Nets "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "ebbfedb7-eab6-49b8-875b-631ddaa4633f",
    "_uuid": "71fc0614bdbf60104024c53c2196d7f342be1819"
   },
   "source": [
    "### 1D convolutional with glove embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "4683d404-a3f1-4980-9213-0525203ac09b",
    "_uuid": "ebf5d2dbf1729ad54bc58162c461b7af7cb0a89b",
    "collapsed": true
   },
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "mbti_1 = pd.read_csv('data/mbti_1.csv') \n",
    "posts = mbti_1.posts\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = \"data/glove.6B\"\n",
    "TEXT_DATA_DIR = \"data/20_newsgroup\"\n",
    "MAX_SEQUENCE_LENGTH = 923\n",
    "MAX_NB_WORDS = 2000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "be232bcb-f521-4d22-8bb0-8473cec66575",
    "_uuid": "c400b47eb7fd7c0a2255e2a116aa5874d874e103",
    "collapsed": true
   },
   "source": [
    "# build index mapping words in the embeddings set to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.%sd.txt'%str(EMBEDDING_DIM)))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "a851a380-84da-4968-af80-9aad03e4ebe9",
    "_uuid": "85249fa47d748d8e1c07639777518aaebf11e653",
    "collapsed": true
   },
   "source": [
    "# prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = [post.replace(\"link\", \"\") for post in list_posts] # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = np.array(list_personality_bin)\n",
    "# list of label ids\n",
    "\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "f4bedcee-6f07-4e88-9d60-e7eae745f00e",
    "_uuid": "eb07c01928c05b333c69ffac4105285d64f5e4c9",
    "collapsed": true
   },
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "b5e091e6-1179-42f0-b610-fc06a7e69742",
    "_uuid": "c00473738c5ab051c9a8be189230b512764f814f",
    "collapsed": true
   },
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "7397bfba-ce15-4645-90f9-7d8cd88a7002",
    "_uuid": "0d071eced7a1e160cf3f0cb621bb8f468dac4c66",
    "collapsed": true
   },
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "843722cc-2c00-4551-8015-174187c51304",
    "_uuid": "dd91a31bf2389a69319c6b9030beeeb670fa9ac9"
   },
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "22b1fc9a-d5c6-4cce-8791-56e31b6e3224",
    "_uuid": "5e804b33d1d1a5ab181f8439a13fed6d23618dc8",
    "collapsed": true
   },
   "source": [
    "print('Training convolutional network.')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(64, 4, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(64, 4, activation='relu')(x)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(64, 4, activation='relu')(x)\n",
    "x = MaxPooling1D(25)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "preds = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "86807719-597f-45f0-bed0-995b60b427d1",
    "_uuid": "78d720031bc26d6946c8ee6915c11586a141fe47",
    "collapsed": true
   },
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "141248b8-2418-440f-b88d-e8bb31ced809",
    "_uuid": "b09fe4681bc7c7af0d4d246d51416fbd511f9eef",
    "collapsed": true
   },
   "source": [
    "# Summer is coming!\n",
    "print('Training convolutional network.')\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "98f1649b-0383-4759-8ba1-a067899cdd86",
    "_uuid": "3b816bd46594d3cfc82a43bc540614cfbde46828"
   },
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "4f8dd333-b0d1-49e8-947a-e76fa2348647",
    "_uuid": "82ca1480a501bd57c947bcacfa439ebab353c712",
    "collapsed": true
   },
   "source": [
    "# Bidirectional LSTM\n",
    "\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list_posts)\n",
    "sequences = tokenizer.texts_to_sequences(list_posts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "ddc2d9f3-96a8-43d4-ba5a-b4230817d8a0",
    "_uuid": "cbbb4da7879a3c3c3c3e3ae091a9d9fb99ae49ef",
    "collapsed": true
   },
   "source": [
    "# split the data into a training set and a validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_test = train_test_split(sequences, list_personality_bin, test_size=0.3, random_state=0, stratify=list_personality)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_sentence_length = 600\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_val, maxlen=max_review_length)\n",
    "\n",
    "max_features = 2000\n",
    "batch_size = 32\n",
    "\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 256, input_length=max_review_length))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[X_test, y_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

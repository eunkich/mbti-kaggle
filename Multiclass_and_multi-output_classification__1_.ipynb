{"cells":[{"metadata":{"_uuid":"d42dbe67549b6bb80d80d2ff8b613733f1c84d30","_cell_guid":"8eb608e3-4a08-4962-be7e-f53ea5aa1421"},"cell_type":"markdown","source":"This kernel explores the Myers-Briggs Personality Type Dataset. The data was extracted from comments let by users with different MBTI personality.\n\nI used a simple model which trains relatively quickly. The cross validation output an f1-score of ~0.65. It is possible to search for hyper paramaters with bayesian optimisation to improve accuracy. Also, a pre-trained words embedding like glove in an neural model may bring more insight. A lot of the links are dead links but it would be interesting to scrap the different urls to extract more information. \n\n# Text Analysis with (MBTI) Myers-Briggs Personality Type Dataset\n\n## Description \n\n\n\nIi is implemented in Python 3.6.\n\nThe main steps are :\n\n1. Treat data\n* Load data using pandas\n* Split each row by type | comments \n* Cleaning each comments by :\n    * removing all urls replaced by dummy word 'link' -> erase (replace by '')\n    * removing everything except letters\n    * removing larger spaces and lowering\n    * removing stopwords and lemmatizing\n    * Labelizing each mbti personality\n\n2. Learning phase:\n    - 16 classes (mutliclass):\n        * Run tf-idf and count vectorization of words vectorization \n        * Compute pca, fast\\_ica, tsne\n        * Multinomial Naive Bayes (F1-score ~ 0.603) [5 fold stratified cross validation]\n        * XGboost (F1-score ~ 0.656) [5 fold stratified cross validation]\n        \n    - 4 binary classes (mutlioutput binary classification):\n        * 4 Adaboost model with a multioutput classifier\n        \nAs the letters are only paired by two, the problem is to predict 4 binary variables. The cross validation gives the following table. \n          \n|Attribute| Extraversion (E) - Introversion (I)  | Sensation (S) - INtuition (N)  | Thinking (T) - Feeling (F)  | Judgement (J) - Perception (P)  |   \n|:-:|---|---|---|---|\n|F1-Score|0.59 | 0.44 | 0.8 |  0.83| \n\nThis allows us to see what are the axes that the algorithm may fail at identifying (S Sensation vs N Intuition).\n\nFinally there are two neural networks, (1D convnet and LSTM) but I did not have the time to tune them.\n\nTO DO:\n- Scrape urls and extract topic of pages\n- Balance the dataset with resampling techniques\n- Hyper Parameter Optimization\n- Convolutional Network with glove embedding\n- Search multiple classifiers (not correlated) and stack them and build pipeline with Lasso.","execution_count":null},{"metadata":{"_uuid":"a0f7e95a8994cea779577c48c3d22cde3fdf2cc5","_cell_guid":"0e10c6de-685a-4e93-9925-7d73cb27497c","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import *\nimport seaborn as sns\n\n%matplotlib inline\n\n# read data\ndata = pd.read_csv('../input/mbti_1.csv') # dtype = {'type': str,'post': ,","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840ae5aa20a5c29b983be8c45928862c299b9792","_cell_guid":"376de180-026c-46c1-8dfa-e2550edd0955"},"cell_type":"markdown","source":"## Overview of the data","execution_count":null},{"metadata":{"_uuid":"03a18534be1141ebff5875c48f7075995f23ca60","_cell_guid":"315f710c-371f-4c9d-87f0-13fbc0e29047","trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d145f588ec28c76d0c757d53e3b657e21c4e7a9","_cell_guid":"bda2db8c-c9c6-446b-a40e-f86241d60619"},"cell_type":"markdown","source":"### List of posts","execution_count":null},{"metadata":{"_uuid":"14c6ff28cd51d9969324fecb6a53b05ca5584069","_cell_guid":"05abe1d2-7acd-4f20-9539-be413c76822b","trusted":true},"cell_type":"code","source":"[p.split('|||') for p in data.head(1).posts.values]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79b9bb46f0aeba9d734a35bfdffedb7ad90ff51c","_cell_guid":"24c37003-e27e-4278-aa92-0884c595f1b3"},"cell_type":"markdown","source":"# Distribution of the target variable?","execution_count":null},{"metadata":{"_uuid":"2fc67d1db4ef61d5998b262ea0cf4eae9bbe8594","_cell_guid":"1abc712c-49df-4316-ae70-c2e6a68c11e0","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(40,20))\nplt.xticks(fontsize=24, rotation=0)\nplt.yticks(fontsize=24, rotation=0)\nsns.countplot(data=data, x='type')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eefe6923a02aeeefc0dcde74ee0a914f41bb73b","_cell_guid":"ad38422a-6742-41f5-ab7c-13f3dd78880b"},"cell_type":"markdown","source":"#### It is clearly unbalanced throughout the different classes. ","execution_count":null},{"metadata":{"_uuid":"1832a12c5350356c50d8664aeb4cecd68adc6026","_cell_guid":"dbcabf3e-06a9-4bf4-88d8-a16690a12c11"},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"_uuid":"196c5662911d0f7c33a49773138c6506d9366fdf","_cell_guid":"a2fbfbc6-4aea-4202-acd5-ad5a07d61fb4"},"cell_type":"markdown","source":"### Label Encoding","execution_count":null},{"metadata":{"_uuid":"a66303b0a5f658ae3a4852224659a706879b59bc","_cell_guid":"99d84362-6521-45a4-9c14-3d236d154bae","trusted":true},"cell_type":"code","source":"##### Encode each type to an int\n\nfrom sklearn.preprocessing import LabelEncoder\n\nunique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\nlab_encoder = LabelEncoder().fit(unique_type_list)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dff206a4e913892839455b75ffc22de56320743","_cell_guid":"5840c3c9-2e64-4e85-bdb5-cddd64c17ebc"},"cell_type":"markdown","source":"### Posts cleaning","execution_count":null},{"metadata":{"_uuid":"29ccab43c1c8eb2ab4caa3085513f7e56d69c7a4","_cell_guid":"84616491-994b-4659-b515-9b92fe707add","trusted":true},"cell_type":"code","source":"import time\n##### Compute list of subject with Type | list of comments \n\n# Time\n%time data.posts[1].replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ')\n%time re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', data.posts[1])\n\n\nfrom nltk.corpus import stopwords \nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Lemmatizer | Stemmatizer\nstemmer = PorterStemmer()\nlemmatiser = WordNetLemmatizer()\n\n# Cache the stop words for speed \ncachedStopWords = stopwords.words(\"english\")\n\n# One post\nOnePost = data.posts[1]\n\n# List all urls\nurls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', OnePost)\n\n# Remove urls\ntemp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', OnePost)\n\n# Keep only words\ntemp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n\n# Remove spaces > 1\ntemp = re.sub(' +', ' ', temp).lower()\n\n# Remove stopwords and lematize\n%time stemmer.stem(\" \".join([w for w in temp.split(' ') if w not in cachedStopWords]))\n\nprint(\"\\nBefore preprocessing:\\n\\n\", OnePost[0:500])\nprint(\"\\nAfter preprocessing:\\n\\n\", temp[0:500])\nprint(\"\\nList of urls:\")\nurls","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6d6b98ad50f4590bb790f31ab275ac3eab7c169","_cell_guid":"7902b557-a441-4524-8e10-8b7c463b8c5a"},"cell_type":"markdown","source":"### Preprocessing comments\n\n* Replace urls with a dummy word: \"link\"\n* Keep only words and put everything lowercase\n* Lemmatize each word \n","execution_count":null},{"metadata":{"_uuid":"3bd6a4ff38bbe36b4f80b92d66c67cf9d8acce02","_cell_guid":"733a2aed-6588-4948-9736-4efa07188943","trusted":true},"cell_type":"code","source":"##### Compute list of subject with Type | list of comments \nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Lemmatize\nstemmer = PorterStemmer()\nlemmatiser = WordNetLemmatizer()\n\ndef pre_process_data(data, remove_stop_words=True):\n\n    list_personality = []\n    list_posts = []\n    len_data = len(data)\n    i=0\n    \n    for row in data.iterrows():\n        i+=1\n        if i % 500 == 0:\n            print(\"%s | %s rows\" % (i, len_data))\n\n        ##### Remove and clean comments\n        posts = row[1].posts\n        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', posts)\n        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n        temp = re.sub(' +', ' ', temp).lower()\n        if remove_stop_words:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n        else:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n\n        type_labelized = lab_encoder.transform([row[1].type])[0]\n        list_personality.append(type_labelized)\n        list_posts.append(temp)\n\n    #del data\n    list_posts = np.array(list_posts)\n    list_personality = np.array(list_personality)\n    return list_posts, list_personality\n\nlist_posts, list_personality = pre_process_data(data, remove_stop_words=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50c7d4b18f04b6868bc235b5ef0ac6d5a2d22032","_cell_guid":"5935f4ca-6d8d-40ae-850c-fd4409968fd8"},"cell_type":"markdown","source":"### Vectorize with count and tf-idf\n\nI kept the words appearing in 10 to 50% of the posts. ","execution_count":null},{"metadata":{"_uuid":"fa17dad79c85809225b1ae29b18a3a1501a3f574","_cell_guid":"ea8bb4b1-0f28-4642-924d-4379c778abbf","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\n\ncntizer = CountVectorizer(analyzer=\"word\", \n                             max_features=1500, \n                             tokenizer=None,    \n                             preprocessor=None, \n                             stop_words=None,  \n#                             ngram_range=(1,1),\n                             max_df=0.5,\n                             min_df=0.1) \n                                 \ntfizer = TfidfTransformer()\n\nprint(\"CountVectorizer\")\nX_cnt = cntizer.fit_transform(list_posts)\nprint(\"Tf-idf\")\nX_tfidf =  tfizer.fit_transform(X_cnt).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cnt[0][0][]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"763745c5bf4ccac6aa8c84cb937fb928ce8df647","_cell_guid":"4563d9df-3c32-45fb-ac64-103f22aae383","trusted":true},"cell_type":"code","source":"list_posts[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f02c5786ce7714a179721417e0463117ea23d1e7","_cell_guid":"100f2063-efe5-448c-809a-2ead70190381"},"cell_type":"markdown","source":"### Count the top 50 words of the count vectorizer","execution_count":null},{"metadata":{"_uuid":"36e4d1703e964eb0d8f76e8547e4a6a7c21ad6af","_cell_guid":"ec9b2fa7-da8e-4eea-ac73-e898f9be3943","trusted":true},"cell_type":"code","source":"reverse_dic = {}\nfor key in cntizer.vocabulary_:\n    reverse_dic[cntizer.vocabulary_[key]] = key","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d7c435788ba0b86ddbcbebbd51daf3431ca2f67","_cell_guid":"72ddbfca-a4e4-4506-8474-4345767b9e39","scrolled":true,"trusted":true},"cell_type":"code","source":"top_50 = np.asarray(np.argsort(np.sum(X_cnt, axis=0))[0,-50:][0, ::-1]).flatten()\n[reverse_dic[v] for v in top_50]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aeddf0e1517e02d0b0c4bbe49e518baaa3772f6","_cell_guid":"4f208b76-82df-4383-b6b7-4afb86e75e6c"},"cell_type":"markdown","source":"The 4 top words are actually some mbti profiles... This is strange!\n\nLets try dimensionality reduction:","execution_count":null},{"metadata":{"_uuid":"11b23e9d54343717bcf6fe9da77b9aa90f46f8aa","_cell_guid":"8a707754-fe83-49f4-b561-99942bab5e3f","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n# Truncated SVD\nsvd = TruncatedSVD(n_components=12, n_iter=7, random_state=42)\nsvd_vec = svd.fit_transform(X_tfidf)\n\nprint(\"TSNE\")\nX_tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=650).fit_transform(svd_vec)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5685baf6df806ab6e44531e1031b034b9d1981e","_cell_guid":"23f07804-9233-4a62-b3d2-ca63a62be082"},"cell_type":"markdown","source":"### Plot tsne with 3 components","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tsne","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aee9a6f532c16a8c49d8e0a3b9a46c1576d7fa31","_cell_guid":"500f677f-2f50-46d6-a781-a79cbdff2bff","trusted":true},"cell_type":"code","source":"col = list_personality\nplt.figure(0)\nplt.scatter(X_tsne[:,0], X_tsne[:,1], c=col, cmap=plt.get_cmap('tab20') , s=12)\nplt.figure(1)\nplt.scatter(X_tsne[:,0], X_tsne[:,2], c=col, cmap=plt.get_cmap('tab20') , s=12)\nplt.figure(2)\nplt.scatter(X_tsne[:,1], X_tsne[:,2], c=col, cmap=plt.get_cmap('tab20') , s=12)\nlegend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e617fa8a27d8fa54e46fd86b600cd3e20fa7b30","_cell_guid":"44215420-71e0-4626-96b5-67a99322ee28"},"cell_type":"markdown","source":"### Plot first axes of decomposition","execution_count":null},{"metadata":{"_uuid":"8c8829e9ed4a36eba9cb87774b71d5cacabca9cd","_cell_guid":"a50fbe38-2d0a-47c6-a5b5-35eaa63a41c4","scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import KernelPCA, FastICA, PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# PCA\npca_vec = PCA(n_components=12).fit_transform(X_tfidf)\n\n# ICA\nica_vec = FastICA(n_components=12).fit_transform(X_tfidf)\n\n# Plot\nplt.figure(1)\nplt.scatter(pca_vec[:,0], pca_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)\nplt.figure(2)\nplt.scatter(svd_vec[:,0], svd_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)\nplt.figure(3)\nplt.scatter(ica_vec[:,0], ica_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c216341ece64a8c745a8ca68dfd06fdd9dd9d28","_cell_guid":"26355d5e-3a87-40e1-9b23-819bf8d7c2ec"},"cell_type":"markdown","source":"#### Plot tsne for each pair of letter:\n\n* Extraversion (E) - Introversion (I)\n* Sensation (S) - INtuition (N)\n* Thinking (T) - Feeling (F)\n* Judgement (J) - Perception (P)","execution_count":null},{"metadata":{"_uuid":"85b5d10a691364b224656410220df047b386874c","_cell_guid":"18197bbd-5016-4833-a3cd-e6f45605d30d","scrolled":false,"trusted":true},"cell_type":"code","source":"# Split mbti personality into 4 letters and binarize\ntitles = [\"Extraversion (E) - Introversion (I)\",\n          \"Sensation (S) - INtuition (N)\",\n          \"Thinking (T) - Feeling (F)\",\n          \"Judgement (J) - Perception (P)\"\n         ] \nb_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\nb_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n\ndef translate_personality(personality):\n    '''\n    transform mbti to binary vector\n    '''\n    return [b_Pers[l] for l in personality]\n\ndef translate_back(personality):\n    '''\n    transform binary vector to mbti personality\n    '''\n    s = \"\"\n    for i, l in enumerate(personality):\n        s += b_Pers_list[i][l]\n    return s\n\nlist_personality_bin = np.array([translate_personality(p) for p in data.type])\nprint(\"Binarize MBTI list: \\n%s\" % list_personality_bin)\n\n# Plot\ndef plot_tsne(X, i):\n    a = plt.figure(i, figsize=(30,20))\n    plt.title(titles[i])\n    plt.subplot(3,1,1)\n    plt.scatter(X[:,0], X[:,1], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)\n    plt.subplot(3,1,2)\n    plt.scatter(X[:,0], X[:,2], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)\n    plt.subplot(3,1,3)\n    plt.scatter(X[:,1], X[:,2], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f4d91d2d1fa8cca42a42605843da58435e2c7e0","_cell_guid":"e0790fa1-adbd-4ba0-b541-c677ad25a50b"},"cell_type":"markdown","source":"#### Extraversion (E) - Introversion (I)","execution_count":null},{"metadata":{"_uuid":"4dda84494d50afe4474538902739883a79e80ead","_cell_guid":"fe876c32-d095-49f7-b935-2ec5103765c2","trusted":true},"cell_type":"code","source":"plot_tsne(X_tsne, 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f66a0097ea59504595729c39b05908345048b885","_cell_guid":"fae26bc6-a062-4950-a1bd-8ccf5ebb8fcb"},"cell_type":"markdown","source":"#### Sensation (S) - INtuition (N)","execution_count":null},{"metadata":{"_uuid":"b1e233fbae9e7a236f237e8389356da10169acd0","_cell_guid":"f2cdc32a-77e1-428a-8e22-e2967535edf9","trusted":true},"cell_type":"code","source":"plot_tsne(X_tsne, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6459545fc7e2f21a83caee43feeea35ab52febb","_cell_guid":"f3084bf5-cdbc-40d0-8bce-952cb1ee0a86"},"cell_type":"markdown","source":"#### Thinking (T) - Feeling (F)","execution_count":null},{"metadata":{"_uuid":"9366f085027c903afd09419a3e156b5e9c146970","_cell_guid":"01dfb487-39d0-4a9f-ba40-da670c80beeb","trusted":false},"cell_type":"code","source":"plot_tsne(X_tsne, 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b225a198324fd1da2810245d61a21cf60fa7a9d","_cell_guid":"60b5940a-7afd-4676-90f6-214562987c6a"},"cell_type":"markdown","source":"#### Judgement (J) - Perception (P)","execution_count":null},{"metadata":{"_uuid":"d02a90c52b7eaadec522f160d0d3935034ba61bb","_cell_guid":"06c08c87-fbb0-43a9-bfb8-a9a4b52a7c3f","trusted":false},"cell_type":"code","source":"plot_tsne(X_tsne, 3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19399ccc46cefc8b7533471d75ae6b76333bbf38","_cell_guid":"6d842d66-e53e-486d-a10d-c66e3c7f1bce"},"cell_type":"markdown","source":"#### Confusion plot function","execution_count":null},{"metadata":{"_uuid":"f96f3440b4d5e74f11c76e12b3596c820e261c1c","_cell_guid":"f83939a4-139d-4671-a400-f93b7f049478","trusted":true},"cell_type":"code","source":"# Confusion plot\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f3de7448eb2d0f6710d2823c44f7842e3cc2273","_cell_guid":"f777dbca-1af5-41ca-885d-f866d1d0bfdc"},"cell_type":"markdown","source":"### Try multiple sklearn classifiers","execution_count":null},{"metadata":{"_uuid":"92222f6f57dab9f7bd9135c29d6de9c9e0ad6d50","collapsed":true,"_cell_guid":"61ae0dd4-59ea-4dde-92c4-e7e09358abed","trusted":false},"cell_type":"code","source":"##### Sklearn classifiers\n\nfrom sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier\nfrom sklearn.svm import LinearSVC, NuSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n\nimport xgboost as xgb\nimport pickle\n\n# Vectorizer\n\ncntizer = CountVectorizer(analyzer=\"word\", \n                             max_features=1000, \n                             tokenizer=None,    \n                             preprocessor=None, \n                             stop_words=None,   \n                             max_df=0.5,\n                             min_df=0.1) \n\ntfizer = TfidfTransformer()\n\n# Classifiers\nPassAgg = PassiveAggressiveClassifier(max_iter=50)\n\nsgd = SGDClassifier(loss='hinge',   \n              penalty='l1',   \n              alpha=1e-2,     \n              random_state=42,\n              max_iter=7,     \n              tol=None)\n\n# SVM\nlsvc = LinearSVC()\n\n# Multinomial Naive Bayes\nmlNB = MultinomialNB()\n\n# Xgboost \n# setup parameters for xgboost\nparam = {}\n\n# use softmax multi-class classification\nparam['objective'] = 'multi:softprob'\n# scale weight of positive examples\nparam['eta'] = 0.6\nparam['ntrees'] = 300\nparam['subsample'] = 0.93\nparam['max_depth'] = 2\nparam['silent'] = 1\nparam['n_jobs'] = 8\nparam['num_class'] = len(unique_type_list)\nxgb_class = xgb.XGBClassifier(**param)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"113df75f59d260437089bb76b70484ede6f74fbd","_cell_guid":"829a1c71-11a5-4373-bdf0-52b30bd47e60"},"cell_type":"markdown","source":"### Stratified K-fold validation training","execution_count":null},{"metadata":{"_uuid":"680baf0d4cac90d2ce061e73e49e27f2154fae7d","collapsed":true,"_cell_guid":"f89d9440-a23c-4b0b-9ce4-5548595b5f1e","trusted":false},"cell_type":"code","source":"# Train with k fold stratified validation\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n\nname = lambda x : str(x).split('(')[0]\n\ndef train_stratified(models, X, y, add_idf=False, nsplits=3, confusion=False):\n    '''\n    Take a sklearn model like, feature set X, target set y and number of splits to compute Stratified kfold validation.\n    Args:\n        X (array):       Numpy array of features.\n        y (str):         Target - Personality list.\n        add_idf (bool):  Wehther to use tf-idf on CountVectorizer.\n        nsplits(int):    Number of splits for cross validation.\n        confusion(bool): Wether to plot confusion matrix \n        \n    Returns:\n        dict: Dictionnary of classifiers and their cv f1-score.\n    '''\n    fig_i = 0\n    kf = StratifiedShuffleSplit(n_splits=nsplits)\n    \n    # Store fold score for each classifier in a dictionnary\n    dico_score = {}\n    dico_score['merged'] = 0\n    for model in models:\n        dico_score[name(model)] = 0\n    \n    # Stratified Split\n    for train, test in kf.split(X,y):\n        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n        \n        X_train = cntizer.fit_transform(X_train)\n        X_test = cntizer.transform(X_test)\n        \n        # tf-idf\n        if add_idf == True:\n            X_train_tfidf = tfizer.fit_transform(X_train_cnt)\n            X_test_tfidf = tfizer.transform(X_test_cnt)\n        \n            X_train = np.column_stack((X_train_tfidf.todense(), X_train))\n            X_test = np.column_stack((X_test_tfidf.todense(), X_test))\n        \n        probs = np.ones((len(y_test), 16))\n        for model in models:\n            # if xgboost use dmatrix\n            if 'XGB' in name(model):\n                xg_train = xgb.DMatrix(X_train, label=y_train)\n                xg_test = xgb.DMatrix(X_test, label=y_test)\n                watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n                num_round = 30\n                bst = xgb.train(param, xg_train, num_round, watchlist, early_stopping_rounds=6)\n                preds = bst.predict(xg_test)\n                probs = np.multiply(probs, preds)\n                preds = np.array([np.argmax(prob) for prob in preds])\n            else:\n                model.fit(X_train, y_train)\n                preds = model.predict(X_test)\n                probs = np.multiply(probs, model.predict_proba(X_test))\n            # f1-score\n            score = f1_score(y_test, preds, average='weighted')\n            dico_score[name(model)] += score\n            print('%s : %s' % (str(model).split('(')[0], score))\n            \n            if confusion == True:\n                # Compute confusion matrix\n                cnf_matrix = confusion_matrix(y_test, preds)\n                np.set_printoptions(precision=2)\n                # Plot confusion matrix\n                plt.figure(fig_i)\n                fig_i += 1\n                plot_confusion_matrix(cnf_matrix, classes=lab_encoder.inverse_transform(range(16)), normalize=True,\n                                                          title=('Confusion matrix %s' % name(model)))\n        \n        # product of class probabilites of each classifier \n        merged_preds = [np.argmax(prob) for prob in probs]\n        score = f1_score(y_test, merged_preds, average='weighted')\n        print('Merged score: %s' % score)\n        dico_score['merged'] += score\n        \n    return {k: v / nsplits for k, v in dico_score.items()}\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29e0c7b0a8c475246cdad26791ad6db0a254e096","_cell_guid":"9e6a38f2-7d8e-4512-bdf4-85d230855ec4"},"cell_type":"markdown","source":"#### Compare multinomial naive bayes, xgb and their product predictions","execution_count":null},{"metadata":{"_uuid":"b766aeeede25e41b209e1f6ac8c8499928673e38","_cell_guid":"393632cf-22cf-49b4-97ca-88cb3c6cd2ba","scrolled":false,"trusted":false},"cell_type":"code","source":"results = train_stratified([mlNB, xgb_class], list_posts, list_personality, add_idf=False, nsplits=5, confusion=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da41ee2ae73e7306d62bd8b259da20fc4114058c","_cell_guid":"31e80231-fcea-45ed-b4ba-0a3a2b71576d","trusted":false},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5808ce84c622a30d92f57b2a0a14a394564bd37f","_cell_guid":"ca37c745-8736-4f5c-96ee-e67815141c6a"},"cell_type":"markdown","source":"### Try multioutput classification","execution_count":null},{"metadata":{"_uuid":"4fc1c0c9b6da2b44d709d5d88593a54257697303","_cell_guid":"6fcfa0cb-5639-4c4e-b29f-2e08795c1242","scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\n\nb_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\nb_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n\ndef translate_personality(personality):\n    '''\n    transform mbti to binary vector\n    '''\n    return [b_Pers[l] for l in personality]\n\ndef translate_back(personality):\n    '''\n    transform binary vector to mbti personality\n    '''\n    s = \"\"\n    for i, l in enumerate(personality):\n        s += b_Pers_list[i][l]\n    return s\n\nlist_personality_bin = np.array([translate_personality(p) for p in data.type])\nprint(\"Binarize MBTI list: \\n%s\" % list_personality_bin)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6880e52e889124067e3f1f2cab60a1202027150c","collapsed":true,"_cell_guid":"8c37591b-983b-4044-9244-91980aefd99f","trusted":false},"cell_type":"code","source":"# Feed classifier to MultiOutputCLassifier\n\nclf = AdaBoostClassifier()\nmulti_target_classifier = MultiOutputClassifier(clf, n_jobs=-1)\nmulti_target_classifier.fit(X_tfidf, list_personality_bin)\npreds = multi_target_classifier.predict(X_tfidf)\n\npreds_t = [translate_back(p) for p in preds]\nvec1 = data.type ==  preds_t\nfor i in range(4):\n    print(\"f1 score for %s:\\n%s\" % (titles[i],\n                                    f1_score(np.array(list_personality_bin)[:,i], preds[:,i])))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c06928ae12a17e4f7c08b32fe6fa3b945714d11f","_cell_guid":"6cbbdf3c-e9cc-4799-ac9a-b4797660775a","scrolled":true,"trusted":false},"cell_type":"code","source":"# Stratified cross val for multi-output\nX = list_posts\ny = np.array(list_personality_bin)\n\nclf = AdaBoostClassifier()\n\nkf = StratifiedShuffleSplit(n_splits=4)\n\nlist_score = []\nlist_score_per_class= []\n\nfor train, test in kf.split(X, y):\n    X_train, X_test, y_train, y_test = \\\n        X[train], X[test], y[train], y[test]\n\n    X_train = cntizer.fit_transform(X_train)\n    X_test = cntizer.transform(X_test)\n    \n    X_train = tfizer.fit_transform(X_train).toarray()\n    X_test = tfizer.transform(X_test).toarray()\n\n    multi_target_classifier = MultiOutputClassifier(clf, n_jobs=-1)\n    multi_target_classifier.fit(X_train, y_train)\n    preds = multi_target_classifier.predict(X_test)\n    \n    rev_preds = np.array([translate_back(p) for p in preds]) \n    rev_test = np.array([translate_back(p) for p in y_test])\n    score = f1_score(rev_test,rev_preds, average='weighted')\n    list_score.append(score)\n    print('\\nTotal score: %s' % f1_score(rev_test,rev_preds, average='weighted'))\n\n    list_temp =[]\n    for i in range(4):\n        score_per_class = f1_score(y_test[:,i], preds[:,i])\n        list_temp.append(score_per_class)\n        print(score_per_class)\n    list_score_per_class.append(list_temp)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e8a32f1524a3e0c4d6a181b399255859923efc4","_cell_guid":"b5464e41-7ae9-4d2e-b847-ab1ce17955fd","trusted":false},"cell_type":"code","source":"list_score_per_class = np.array(list_score_per_class)\nprint('Mean score per classes: %s' % list_score_per_class.mean(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1521d625c318866b593c3b1dcd39042c00bbff0","_cell_guid":"ce8bacb9-9418-4bec-968e-252eb083b30b"},"cell_type":"markdown","source":"## Neural Nets ","execution_count":null},{"metadata":{"_uuid":"71fc0614bdbf60104024c53c2196d7f342be1819","_cell_guid":"ebbfedb7-eab6-49b8-875b-631ddaa4633f"},"cell_type":"markdown","source":"### 1D convolutional with glove embedding","execution_count":null},{"metadata":{"_uuid":"ebf5d2dbf1729ad54bc58162c461b7af7cb0a89b","collapsed":true,"_cell_guid":"4683d404-a3f1-4980-9213-0525203ac09b","trusted":false},"cell_type":"code","source":"from __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\nfrom keras.models import Model\n\nmbti_1 = pd.read_csv('data/mbti_1.csv') \nposts = mbti_1.posts\n\nBASE_DIR = ''\nGLOVE_DIR = \"data/glove.6B\"\nTEXT_DATA_DIR = \"data/20_newsgroup\"\nMAX_SEQUENCE_LENGTH = 923\nMAX_NB_WORDS = 2000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c400b47eb7fd7c0a2255e2a116aa5874d874e103","collapsed":true,"_cell_guid":"be232bcb-f521-4d22-8bb0-8473cec66575","trusted":false},"cell_type":"code","source":"# build index mapping words in the embeddings set to their embedding vector\n\nprint('Indexing word vectors.')\n\nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.%sd.txt'%str(EMBEDDING_DIM)))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85249fa47d748d8e1c07639777518aaebf11e653","collapsed":true,"_cell_guid":"a851a380-84da-4968-af80-9aad03e4ebe9","trusted":false},"cell_type":"code","source":"# prepare text samples and their labels\nprint('Processing text dataset')\n\ntexts = [post.replace(\"link\", \"\") for post in list_posts] # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = np.array(list_personality_bin)\n# list of label ids\n\n\nprint('Found %s texts.' % len(texts))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb07c01928c05b333c69ffac4105285d64f5e4c9","collapsed":true,"_cell_guid":"f4bedcee-6f07-4e88-9d60-e7eae745f00e","trusted":false},"cell_type":"code","source":"# vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c00473738c5ab051c9a8be189230b512764f814f","collapsed":true,"_cell_guid":"b5e091e6-1179-42f0-b610-fc06a7e69742","trusted":false},"cell_type":"code","source":"# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-num_validation_samples]\ny_train = labels[:-num_validation_samples]\nx_val = data[-num_validation_samples:]\ny_val = labels[-num_validation_samples:]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d071eced7a1e160cf3f0cb621bb8f468dac4c66","collapsed":true,"_cell_guid":"7397bfba-ce15-4645-90f9-7d8cd88a7002","trusted":false},"cell_type":"code","source":"print('Preparing embedding matrix.')\n\n# prepare embedding matrix\nnum_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i >= MAX_NB_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# load pre-trained word embeddings into an Embedding layer\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd91a31bf2389a69319c6b9030beeeb670fa9ac9","_cell_guid":"843722cc-2c00-4551-8015-174187c51304"},"cell_type":"markdown","source":"### Define network","execution_count":null},{"metadata":{"_uuid":"5e804b33d1d1a5ab181f8439a13fed6d23618dc8","collapsed":true,"_cell_guid":"22b1fc9a-d5c6-4cce-8791-56e31b6e3224","trusted":false},"cell_type":"code","source":"print('Training convolutional network.')\n\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(64, 4, activation='relu')(embedded_sequences)\nx = MaxPooling1D(4)(x)\nx = Conv1D(64, 4, activation='relu')(x)\nx = MaxPooling1D(4)(x)\nx = Conv1D(64, 4, activation='relu')(x)\nx = MaxPooling1D(25)(x)  # global max pooling\nx = Flatten()(x)\nx = Dense(64, activation='relu')(x)\npreds = Dense(4, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78d720031bc26d6946c8ee6915c11586a141fe47","collapsed":true,"_cell_guid":"86807719-597f-45f0-bed0-995b60b427d1","trusted":false},"cell_type":"code","source":"from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b09fe4681bc7c7af0d4d246d51416fbd511f9eef","collapsed":true,"_cell_guid":"141248b8-2418-440f-b88d-e8bb31ced809","trusted":false},"cell_type":"code","source":"# Summer is coming!\nprint('Training convolutional network.')\n\nmodel.fit(x_train, y_train, validation_data=(x_val, y_val),\n          epochs=100, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b816bd46594d3cfc82a43bc540614cfbde46828","_cell_guid":"98f1649b-0383-4759-8ba1-a067899cdd86"},"cell_type":"markdown","source":"### Bidirectional LSTM","execution_count":null},{"metadata":{"_uuid":"82ca1480a501bd57c947bcacfa439ebab353c712","collapsed":true,"_cell_guid":"4f8dd333-b0d1-49e8-947a-e76fa2348647","trusted":false},"cell_type":"code","source":"# Bidirectional LSTM\n\nimport numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\n\nVALIDATION_SPLIT = 0.2\nMAX_NB_WORDS = 20000\n\n# fix random seed for reproducibility\nnumpy.random.seed(7)\n\n# Tokenize\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(list_posts)\nsequences = tokenizer.texts_to_sequences(list_posts)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbbb4da7879a3c3c3c3e3ae091a9d9fb99ae49ef","collapsed":true,"_cell_guid":"ddc2d9f3-96a8-43d4-ba5a-b4230817d8a0","trusted":false},"cell_type":"code","source":"# split the data into a training set and a validation set\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_test = train_test_split(sequences, list_personality_bin, test_size=0.3, random_state=0, stratify=list_personality)\n\n# truncate and pad input sequences\nmax_sentence_length = 600\nX_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(x_val, maxlen=max_review_length)\n\nmax_features = 2000\nbatch_size = 32\n\nprint('x_train shape:', X_train.shape)\nprint('x_test shape:', X_test.shape)\n\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 256, input_length=max_review_length))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, y_train,\n          batch_size=batch_size,\n          epochs=4,\n          validation_data=[X_test, y_test])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}